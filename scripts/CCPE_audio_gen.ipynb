{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:09.820588Z",
     "start_time": "2024-12-04T02:12:39.080919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from TTS.api import TTS\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "import sys\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:09.859018Z",
     "start_time": "2024-12-04T02:13:09.822102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "# Initialize the ModelManager\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# List available models\n",
    "available_models = model_manager.list_models()\n",
    "print(available_models)\n",
    "en_tacotron2_models = [model for model in available_models if \"en\" in model and \"tacotron2\" in model]\n",
    "print(en_tacotron2_models)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: tts_models/multilingual/multi-dataset/xtts_v2\n",
      " 2: tts_models/multilingual/multi-dataset/xtts_v1.1\n",
      " 3: tts_models/multilingual/multi-dataset/your_tts\n",
      " 4: tts_models/multilingual/multi-dataset/bark\n",
      " 5: tts_models/bg/cv/vits\n",
      " 6: tts_models/cs/cv/vits\n",
      " 7: tts_models/da/cv/vits\n",
      " 8: tts_models/et/cv/vits\n",
      " 9: tts_models/ga/cv/vits\n",
      " 10: tts_models/en/ek1/tacotron2 [already downloaded]\n",
      " 11: tts_models/en/ljspeech/tacotron2-DDC [already downloaded]\n",
      " 12: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
      " 13: tts_models/en/ljspeech/glow-tts\n",
      " 14: tts_models/en/ljspeech/speedy-speech\n",
      " 15: tts_models/en/ljspeech/tacotron2-DCA [already downloaded]\n",
      " 16: tts_models/en/ljspeech/vits\n",
      " 17: tts_models/en/ljspeech/vits--neon\n",
      " 18: tts_models/en/ljspeech/fast_pitch\n",
      " 19: tts_models/en/ljspeech/overflow\n",
      " 20: tts_models/en/ljspeech/neural_hmm\n",
      " 21: tts_models/en/vctk/vits\n",
      " 22: tts_models/en/vctk/fast_pitch\n",
      " 23: tts_models/en/sam/tacotron-DDC\n",
      " 24: tts_models/en/blizzard2013/capacitron-t2-c50\n",
      " 25: tts_models/en/blizzard2013/capacitron-t2-c150_v2\n",
      " 26: tts_models/en/multi-dataset/tortoise-v2\n",
      " 27: tts_models/en/jenny/jenny\n",
      " 28: tts_models/es/mai/tacotron2-DDC\n",
      " 29: tts_models/es/css10/vits\n",
      " 30: tts_models/fr/mai/tacotron2-DDC\n",
      " 31: tts_models/fr/css10/vits\n",
      " 32: tts_models/uk/mai/glow-tts\n",
      " 33: tts_models/uk/mai/vits\n",
      " 34: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
      " 35: tts_models/nl/mai/tacotron2-DDC\n",
      " 36: tts_models/nl/css10/vits\n",
      " 37: tts_models/de/thorsten/tacotron2-DCA\n",
      " 38: tts_models/de/thorsten/vits\n",
      " 39: tts_models/de/thorsten/tacotron2-DDC\n",
      " 40: tts_models/de/css10/vits-neon\n",
      " 41: tts_models/ja/kokoro/tacotron2-DDC\n",
      " 42: tts_models/tr/common-voice/glow-tts\n",
      " 43: tts_models/it/mai_female/glow-tts\n",
      " 44: tts_models/it/mai_female/vits\n",
      " 45: tts_models/it/mai_male/glow-tts\n",
      " 46: tts_models/it/mai_male/vits\n",
      " 47: tts_models/ewe/openbible/vits\n",
      " 48: tts_models/hau/openbible/vits\n",
      " 49: tts_models/lin/openbible/vits\n",
      " 50: tts_models/tw_akuapem/openbible/vits\n",
      " 51: tts_models/tw_asante/openbible/vits\n",
      " 52: tts_models/yor/openbible/vits\n",
      " 53: tts_models/hu/css10/vits\n",
      " 54: tts_models/el/cv/vits\n",
      " 55: tts_models/fi/css10/vits\n",
      " 56: tts_models/hr/cv/vits\n",
      " 57: tts_models/lt/cv/vits\n",
      " 58: tts_models/lv/cv/vits\n",
      " 59: tts_models/mt/cv/vits\n",
      " 60: tts_models/pl/mai_female/vits\n",
      " 61: tts_models/pt/cv/vits\n",
      " 62: tts_models/ro/cv/vits\n",
      " 63: tts_models/sk/cv/vits\n",
      " 64: tts_models/sl/cv/vits\n",
      " 65: tts_models/sv/cv/vits\n",
      " 66: tts_models/ca/custom/vits\n",
      " 67: tts_models/fa/custom/glow-tts\n",
      " 68: tts_models/bn/custom/vits-male\n",
      " 69: tts_models/bn/custom/vits-female\n",
      " 70: tts_models/be/common-voice/glow-tts\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: vocoder_models/universal/libri-tts/wavegrad\n",
      " 2: vocoder_models/universal/libri-tts/fullband-melgan\n",
      " 3: vocoder_models/en/ek1/wavegrad [already downloaded]\n",
      " 4: vocoder_models/en/ljspeech/multiband-melgan [already downloaded]\n",
      " 5: vocoder_models/en/ljspeech/hifigan_v2 [already downloaded]\n",
      " 6: vocoder_models/en/ljspeech/univnet\n",
      " 7: vocoder_models/en/blizzard2013/hifigan_v2\n",
      " 8: vocoder_models/en/vctk/hifigan_v2\n",
      " 9: vocoder_models/en/sam/hifigan_v2\n",
      " 10: vocoder_models/nl/mai/parallel-wavegan\n",
      " 11: vocoder_models/de/thorsten/wavegrad\n",
      " 12: vocoder_models/de/thorsten/fullband-melgan\n",
      " 13: vocoder_models/de/thorsten/hifigan_v1\n",
      " 14: vocoder_models/ja/kokoro/hifigan_v1\n",
      " 15: vocoder_models/uk/mai/multiband-melgan\n",
      " 16: vocoder_models/tr/common-voice/hifigan\n",
      " 17: vocoder_models/be/common-voice/hifigan\n",
      "\n",
      " Name format: type/language/dataset/model\n",
      " 1: voice_conversion_models/multilingual/vctk/freevc24\n",
      "['tts_models/multilingual/multi-dataset/xtts_v2', 'tts_models/multilingual/multi-dataset/xtts_v1.1', 'tts_models/multilingual/multi-dataset/your_tts', 'tts_models/multilingual/multi-dataset/bark', 'tts_models/bg/cv/vits', 'tts_models/cs/cv/vits', 'tts_models/da/cv/vits', 'tts_models/et/cv/vits', 'tts_models/ga/cv/vits', 'tts_models/en/ek1/tacotron2', 'tts_models/en/ljspeech/tacotron2-DDC', 'tts_models/en/ljspeech/tacotron2-DDC_ph', 'tts_models/en/ljspeech/glow-tts', 'tts_models/en/ljspeech/speedy-speech', 'tts_models/en/ljspeech/tacotron2-DCA', 'tts_models/en/ljspeech/vits', 'tts_models/en/ljspeech/vits--neon', 'tts_models/en/ljspeech/fast_pitch', 'tts_models/en/ljspeech/overflow', 'tts_models/en/ljspeech/neural_hmm', 'tts_models/en/vctk/vits', 'tts_models/en/vctk/fast_pitch', 'tts_models/en/sam/tacotron-DDC', 'tts_models/en/blizzard2013/capacitron-t2-c50', 'tts_models/en/blizzard2013/capacitron-t2-c150_v2', 'tts_models/en/multi-dataset/tortoise-v2', 'tts_models/en/jenny/jenny', 'tts_models/es/mai/tacotron2-DDC', 'tts_models/es/css10/vits', 'tts_models/fr/mai/tacotron2-DDC', 'tts_models/fr/css10/vits', 'tts_models/uk/mai/glow-tts', 'tts_models/uk/mai/vits', 'tts_models/zh-CN/baker/tacotron2-DDC-GST', 'tts_models/nl/mai/tacotron2-DDC', 'tts_models/nl/css10/vits', 'tts_models/de/thorsten/tacotron2-DCA', 'tts_models/de/thorsten/vits', 'tts_models/de/thorsten/tacotron2-DDC', 'tts_models/de/css10/vits-neon', 'tts_models/ja/kokoro/tacotron2-DDC', 'tts_models/tr/common-voice/glow-tts', 'tts_models/it/mai_female/glow-tts', 'tts_models/it/mai_female/vits', 'tts_models/it/mai_male/glow-tts', 'tts_models/it/mai_male/vits', 'tts_models/ewe/openbible/vits', 'tts_models/hau/openbible/vits', 'tts_models/lin/openbible/vits', 'tts_models/tw_akuapem/openbible/vits', 'tts_models/tw_asante/openbible/vits', 'tts_models/yor/openbible/vits', 'tts_models/hu/css10/vits', 'tts_models/el/cv/vits', 'tts_models/fi/css10/vits', 'tts_models/hr/cv/vits', 'tts_models/lt/cv/vits', 'tts_models/lv/cv/vits', 'tts_models/mt/cv/vits', 'tts_models/pl/mai_female/vits', 'tts_models/pt/cv/vits', 'tts_models/ro/cv/vits', 'tts_models/sk/cv/vits', 'tts_models/sl/cv/vits', 'tts_models/sv/cv/vits', 'tts_models/ca/custom/vits', 'tts_models/fa/custom/glow-tts', 'tts_models/bn/custom/vits-male', 'tts_models/bn/custom/vits-female', 'tts_models/be/common-voice/glow-tts', 'vocoder_models/universal/libri-tts/wavegrad', 'vocoder_models/universal/libri-tts/fullband-melgan', 'vocoder_models/en/ek1/wavegrad', 'vocoder_models/en/ljspeech/multiband-melgan', 'vocoder_models/en/ljspeech/hifigan_v2', 'vocoder_models/en/ljspeech/univnet', 'vocoder_models/en/blizzard2013/hifigan_v2', 'vocoder_models/en/vctk/hifigan_v2', 'vocoder_models/en/sam/hifigan_v2', 'vocoder_models/nl/mai/parallel-wavegan', 'vocoder_models/de/thorsten/wavegrad', 'vocoder_models/de/thorsten/fullband-melgan', 'vocoder_models/de/thorsten/hifigan_v1', 'vocoder_models/ja/kokoro/hifigan_v1', 'vocoder_models/uk/mai/multiband-melgan', 'vocoder_models/tr/common-voice/hifigan', 'vocoder_models/be/common-voice/hifigan', 'voice_conversion_models/multilingual/vctk/freevc24']\n",
      "['tts_models/en/ek1/tacotron2', 'tts_models/en/ljspeech/tacotron2-DDC', 'tts_models/en/ljspeech/tacotron2-DDC_ph', 'tts_models/en/ljspeech/tacotron2-DCA', 'tts_models/de/thorsten/tacotron2-DCA', 'tts_models/de/thorsten/tacotron2-DDC']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:09.865298Z",
     "start_time": "2024-12-04T02:13:09.860507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SuppressTTSOutput:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        sys.stdout.close()\n",
    "        sys.stderr.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        sys.stderr = self._original_stderr"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:10.002560Z",
     "start_time": "2024-12-04T02:13:09.866307Z"
    }
   },
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5071, 0.4838, 0.1513],\n",
      "        [0.6296, 0.7884, 0.4236],\n",
      "        [0.2962, 0.5414, 0.2672],\n",
      "        [0.6511, 0.0221, 0.2305],\n",
      "        [0.5950, 0.9620, 0.5338]])\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:10.009921Z",
     "start_time": "2024-12-04T02:13:10.004991Z"
    }
   },
   "source": [
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:10.259600Z",
     "start_time": "2024-12-04T02:13:10.010934Z"
    }
   },
   "source": [
    "# Load JSON file\n",
    "file_path = \"../datasets/ccpe-main/data.json\"  # Update this path if necessary\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:12.598453Z",
     "start_time": "2024-12-04T02:13:10.260610Z"
    }
   },
   "source": [
    "# Initialize TTS models for different speakers\n",
    "user_tts_model = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DCA\", progress_bar=True).to(device)\n",
    "agent_tts_model = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=True).to(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DCA is already downloaded.\n",
      " > vocoder_models/en/ljspeech/multiband-melgan is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:C:\\Users\\mackm\\AppData\\Local\\tts\\tts_models--en--ljspeech--tacotron2-DCA\\scale_stats.npy\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\utils\\io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model's reduction rate `r` is set to: 2\n",
      " > Vocoder Model: multiband_melgan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:0\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:C:\\Users\\mackm\\AppData\\Local\\tts\\vocoder_models--en--ljspeech--multiband-melgan\\scale_stats.npy\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: multiband_melgan_generator\n",
      " > Discriminator Model: melgan_multiscale_discriminator\n",
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:12.603994Z",
     "start_time": "2024-12-04T02:13:12.599731Z"
    }
   },
   "source": [
    "# Create output directories\n",
    "audio_output_dir = \"../datasets/ccpe-main/generated_audio\"\n",
    "text_output_dir = \"../datasets/ccpe-main/generated_text\"\n",
    "os.makedirs(audio_output_dir, exist_ok=True)\n",
    "os.makedirs(text_output_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:12.612712Z",
     "start_time": "2024-12-04T02:13:12.605003Z"
    }
   },
   "source": [
    "# Count total utterances by \"USER\" for tqdm\n",
    "total_utterances = sum(\n",
    "    len([u for u in conversation.get(\"utterances\", []) if u[\"speaker\"] == \"USER\"])\n",
    "    for conversation in data\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:12.628231Z",
     "start_time": "2024-12-04T02:13:12.613720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_conversations = []\n",
    "# get the processed audio files\n",
    "for file in os.listdir(audio_output_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        conversation_id = file.split(\".\")[0]\n",
    "        processed_conversations.append(conversation_id)\n",
    "\n",
    "print(f\"Found {len(processed_conversations)} processed conversations.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 501 processed conversations.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:19.577981Z",
     "start_time": "2024-12-04T02:13:12.629239Z"
    }
   },
   "source": [
    "# Function to generate 2 seconds of silence\n",
    "def generate_silence(duration_ms=2000):\n",
    "    return AudioSegment.silent(duration=duration_ms)\n",
    "\n",
    "failed_conversations = []\n",
    "\n",
    "import traceback\n",
    "\n",
    "# Process each conversation and generate combined audio and text files\n",
    "with tqdm(total=len(data), desc=\"Processing Conversations\") as pbar:\n",
    "    for conversation in data:\n",
    "        conversation_id = conversation.get(\"conversationId\", \"unknown_id\")\n",
    "\n",
    "        # Skip if already processed\n",
    "        if conversation_id in processed_conversations:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            utterances = conversation.get(\"utterances\", [])\n",
    "\n",
    "            # Prepare combined text and speaker-separated audio sequence\n",
    "            combined_audio_text = \"\"\n",
    "            combined_text_with_labels = \"\"\n",
    "            speaker_audio_sequence = []\n",
    "\n",
    "            for utterance in utterances:\n",
    "                speaker = utterance[\"speaker\"]\n",
    "                text = utterance[\"text\"]\n",
    "                combined_text_with_labels += f\"{speaker}: {text}\\n\"\n",
    "\n",
    "                # Apply text cleaning (remove non-alphanumeric characters and extra spaces)\n",
    "                text = re.sub(r\"[^a-zA-Z0-9.,!? ]\", \"\", text)\n",
    "                text = re.sub(' +', ' ', text)\n",
    "\n",
    "                combined_audio_text += f\"{text}\\n\"\n",
    "\n",
    "                # Add cleaned text to the audio sequence\n",
    "                speaker_audio_sequence.append({\"text\": text, \"type\": \"utterance\", \"speaker\": speaker})\n",
    "                speaker_audio_sequence.append({\"type\": \"silence\"})  # Add 2 seconds of silence\n",
    "\n",
    "            # Save the text conversation with speaker labels\n",
    "            text_file_path = os.path.join(text_output_dir, f\"{conversation_id}.txt\")\n",
    "            with open(text_file_path, \"w\") as text_file:\n",
    "                text_file.write(combined_text_with_labels.strip())\n",
    "\n",
    "            # Generate audio for the sequence with silence\n",
    "            audio_output = AudioSegment.silent(duration=0)\n",
    "            for item in speaker_audio_sequence:\n",
    "                \n",
    "                if item[\"type\"] == \"utterance\":\n",
    "                    \n",
    "                    # Change TTS voice based on speaker\n",
    "                    if item[\"speaker\"] == \"USER\":\n",
    "                        with SuppressTTSOutput():\n",
    "                            audio_segment = user_tts_model.tts_to_file(item[\"text\"])\n",
    "                            audio_segment= AudioSegment.from_file(audio_segment)\n",
    "                    else:\n",
    "                        with SuppressTTSOutput():\n",
    "                            audio_segment = agent_tts_model.tts_to_file(item[\"text\"])\n",
    "                            audio_segment= AudioSegment.from_file(audio_segment)\n",
    "                elif item[\"type\"] == \"silence\":\n",
    "                    audio_segment = generate_silence()  # Generate 2 seconds of silence\n",
    "                # print(type(audio_segment))\n",
    "                audio_output += audio_segment\n",
    "\n",
    "            audio_file_path = os.path.join(audio_output_dir, f\"{conversation_id}.wav\")\n",
    "            audio_output.export(audio_file_path, format=\"wav\")\n",
    "\n",
    "            # Add conversation ID to processed list\n",
    "            processed_conversations.append(conversation_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing conversation ID {conversation_id}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            failed_conversations.append(conversation_id)\n",
    "    \n",
    "        pbar.update(1)  # Update progress bar\n",
    "# Print out failed conversation IDs at the end\n",
    "if failed_conversations:\n",
    "    print(\"\\nFailed Conversations:\")\n",
    "    for failed_id in failed_conversations:\n",
    "        print(failed_id)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Conversations:   0%|          | 0/502 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\tts\\utils\\text\\characters.py\", line 300, in char_to_id\n",
      "    return self._char_to_id[char]\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^\n",
      "KeyError: '͡'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\tts\\utils\\text\\tokenizer.py\", line 70, in encode\n",
      "    idx = self.characters.char_to_id(char)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\tts\\utils\\text\\characters.py\", line 302, in char_to_id\n",
      "    raise KeyError(f\" [!] {repr(char)} is not in the vocabulary.\") from e\n",
      "KeyError: \" [!] '͡' is not in the vocabulary.\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mackm\\AppData\\Local\\Temp\\ipykernel_15984\\289528417.py\", line 56, in <module>\n",
      "    audio_segment = user_tts_model.tts_to_file(item[\"text\"])\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\api.py\", line 334, in tts_to_file\n",
      "    wav = self.tts(\n",
      "          ^^^^^^^^^\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\api.py\", line 276, in tts\n",
      "    wav = self.synthesizer.tts(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\utils\\synthesizer.py\", line 398, in tts\n",
      "    outputs = synthesis(\n",
      "              ^^^^^^^^^^\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\tts\\utils\\synthesis.py\", line 194, in synthesis\n",
      "    model.tokenizer.text_to_ids(text, language=language_name),\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\tts\\utils\\text\\tokenizer.py\", line 111, in text_to_ids\n",
      "    text = self.encode(text)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mackm\\Documents\\School\\OBS-FALL-24\\CS 534 - Machine Learning\\final project\\LLA-VAP\\venv\\Lib\\site-packages\\TTS\\tts\\utils\\text\\tokenizer.py\", line 76, in encode\n",
      "    print(text)\n",
      "  File \"C:\\Users\\mackm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u025b' in position 1: character maps to <undefined>\n",
      "Processing Conversations: 100%|██████████| 502/502 [00:06<00:00, 72.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing conversation ID CCPE-75731: 'charmap' codec can't encode character '\\u025b' in position 1: character maps to <undefined>\n",
      "\n",
      "Failed Conversations:\n",
      "CCPE-75731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:19.581377Z",
     "start_time": "2024-12-04T02:13:19.578992Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T02:13:19.584993Z",
     "start_time": "2024-12-04T02:13:19.582388Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
